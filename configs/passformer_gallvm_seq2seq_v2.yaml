# LLVM Optimization Sequence Generation Training Configuration

model:
  instbert_id: /home/xucong24/Compiler/checkpoints/modernbert_poj104_mlm      # 预训练的 ModernBERT/InstBERT 模型路径
  inst2vec_tokenizer_id: /home/xucong24/Compiler/checkpoints/Inst2VecTokenizer  # Inst2Vec tokenizer 路径
  opti_seq_tokenizer_id: /home/xucong24/Compiler/checkpoints/OptiSeqTokenizer   # 优化序列 tokenizer 路径
  fusion_method: add

data:
  data_dir: /home/xucong24/Compiler/datasets/ga_llvm_37k
  # tokenized_data_dir: /home/xucong24/Compiler/datasets/ga_llvm_37k_passformer_128_32_tokenized  # 可选，tokenize 后的数据集保存/加载路径
  encoder_maxlen: 1024
  decoder_maxlen: 256
  test_size: 0.2
  split_seed: 42

output:
  base_work_dir: /home/xucong24/Compiler/work_dirs/passformer_gallvm_seq2seq_v2


# GPT-2 Decoder 配置
decoder_cfg:
  vocab_size: 128                 # 会被 decoder tokenizer 的 vocab_size 覆盖
  n_positions: 512                # 最大位置编码长度
  n_embd: 768                     # 嵌入维度（建议与 encoder hidden_size 一致）
  n_layer: 6                      # Transformer 层数
  n_head: 12                      # 注意力头数
  activation_function: gelu_new
  resid_pdrop: 0.1
  embd_pdrop: 0.1
  attn_pdrop: 0.1
  layer_norm_epsilon: 1e-05
  initializer_range: 0.02
  scale_attn_weights: true
  use_cache: true
  bos_token_id: 126
  eos_token_id: 127
  vocab_size: 128
  add_cross_attention: true       # 必须为 true，启用交叉注意力
  
training_args:
  num_train_epochs: 2              # 30 epochs 通常足够，避免过拟合
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8
  eval_strategy: epoch
  save_strategy: epoch
  save_total_limit: 3               # 多保留一个检查点
  load_best_model_at_end: true
  metric_for_best_model: eval_loss  # 用 eval_loss 选最佳模型
  greater_is_better: false          # loss 越小越好
  logging_strategy: steps
  logging_steps: 50                 # 更频繁记录，方便观察曲线
  learning_rate: 5e-5
  warmup_steps: 500
  weight_decay: 0.01
  report_to: tensorboard
  overwrite_output_dir: false

