{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from src.utils.system import read_ir_from_file\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from src.observation.inst2vec import Inst2vecEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/devmap_data'\n",
    "platform = 'all'\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "dense_layer_size = 32\n",
    "print_summary = False\n",
    "out_folder = 'output/inst2vec_for_devmap'\n",
    "num_classes = 2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "emb_path = 'src/observation/inst2vec/pickle/embeddings.pickle'\n",
    "platform2str = {\n",
    "    \"amd\": \"AMD Tahiti 7970\",\n",
    "    \"nvidia\": \"NVIDIA GTX 970\"\n",
    "}\n",
    "\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "assert platform in ['all', 'amd', 'nvidia'], \\\n",
    "    'Choose device among: all, amd, nvidia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载DevMap数据\n",
    "def load_data(data_path, platform):\n",
    "    # Load runtime data\n",
    "    df = pd.read_csv(data_path + \"/cgo17-{}.csv\".format(platform), index_col=0)\n",
    "    print('--- Read data from', data_path)\n",
    "\n",
    "    df[\"bench_data\"] = (\n",
    "        df.loc[df[\"dataset\"] != \"default\", \"benchmark\"]\n",
    "        + str(\"_\")\n",
    "        + df.loc[df[\"dataset\"] != \"default\", \"dataset\"]\n",
    "    )\n",
    "    df.loc[df[\"dataset\"] == \"default\", \"bench_data\"] = df.loc[\n",
    "        df[\"dataset\"] == \"default\", \"benchmark\"\n",
    "    ]\n",
    "\n",
    "    # llvm文件路径\n",
    "    df[\"bench_data_path\"] = data_path + '/kernels_ir/' + df[\"bench_data\"] + str(\".ll\")\n",
    "\n",
    "    # inst2vec编码\n",
    "    input_files = df[\"bench_data_path\"].values  \n",
    "    num_files = len(input_files)\n",
    "    num_unks = 0\n",
    "    seq_lengths = list()\n",
    "    encoder = Inst2vecEncoder()  # inst2vec 编码器\n",
    "    unk_idx = encoder.unknown_vocab_element    \n",
    "    print('--- Preparing to read', num_files, 'input files from folder', data_path + '/kernels_ir/')\n",
    "    seqs = list()\n",
    "\n",
    "    # 遍历文件，读取ir\n",
    "    for i in tqdm(range(num_files), desc='Encoding files'):\n",
    "        file = input_files[i]\n",
    "        if os.path.exists(file):\n",
    "            ir = encoder.preprocess(file)\n",
    "            encode_ir = encoder.encode(ir)  # inst2vec编码\n",
    "            seq_lengths.append(len(encode_ir))\n",
    "            num_unks += encode_ir.count(unk_idx)\n",
    "            seqs.append([int(s) for s in encode_ir])\n",
    "        else:\n",
    "            raise FileNotFoundError('Input file not found: ' + file)\n",
    "\n",
    "    maxlen = max(seq_lengths)\n",
    "    print('Number of benchmark  : {:>5}'.format(num_files))\n",
    "    print('Shortest sequence    : {:>5}'.format(min(seq_lengths)))\n",
    "    print('Longest sequence     : {:>5}'.format(maxlen))\n",
    "    print('Mean sequence length : {:>5} (rounded down)'.format(math.floor(np.mean(seq_lengths))))\n",
    "    print('Number of \\'UNK\\'      : {:>5}'.format(num_unks))\n",
    "    print('Percentage of \\'UNK\\'  : {:>8.4} (% among all stmts)'.format((num_unks*100)/sum(seq_lengths)))\n",
    "    print('\\'UNK\\' index          : {:>5}'.format(unk_idx))\n",
    "\n",
    "    # Padding logic\n",
    "    maxlen = 512\n",
    "    padded_sequences = []\n",
    "    for seq in seqs:\n",
    "        if len(seq) < maxlen:\n",
    "            # Pad sequence if it is shorter than maxlen\n",
    "            seq = seq + [unk_idx] * (maxlen - len(seq))\n",
    "\n",
    "        seq = seq[:512]\n",
    "        padded_sequences.append(seq)\n",
    "\n",
    "    # Convert to np.array\n",
    "    encoded = np.array(padded_sequences)\n",
    "\n",
    "    # aux data\n",
    "    aux_in = np.array([\n",
    "        df[\"transfer\"].values,\n",
    "        df[\"wgsize\"].values,\n",
    "    ]).T\n",
    "    \n",
    "    # 标签\n",
    "    label = np.array([1 if x == \"GPU\" else 0 for x in df[\"oracle\"].values])\n",
    "\n",
    "    return encoded, aux_in, label, encoder.embeddings, df\n",
    "    \n",
    "class DevMapDataset(Dataset):\n",
    "    def __init__(self, sequences, aux_in, y):\n",
    "        super().__init__()\n",
    "        self.sequences = sequences\n",
    "        self.aux_in = aux_in\n",
    "        self.y = y\n",
    "        # self.embeddings = embeddings\n",
    "        # self.embedding_input = self.embeddings[self.sequences]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        seqs = self.sequences[index]\n",
    "        aux = self.aux_in[index]\n",
    "        label = self.y[index]\n",
    "        return seqs, aux, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络结构\n",
    "class DevMapLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_layers, dropout):\n",
    "        super(DevMapLSTM, self).__init__()\n",
    "\n",
    "        with open(emb_path, \"rb\") as f:\n",
    "            embeddings = pickle.load(f)\n",
    "        embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        embedding_matrix_normalized = F.normalize(embeddings, p=2, dim=1)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix_normalized, freeze=False)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, embedding_dim, num_layers,\n",
    "                            bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim * 2, num_classes)\n",
    "        \n",
    "        # self.fc_with_aux = nn.Linear(embedding_dim * 2 + 2, num_classes)\n",
    "    \n",
    "        # self.language_model_out = nn.Linear(embedding_dim, 2)\n",
    "        self.batch_norm = nn.BatchNorm1d(embedding_dim * 2 + 2)\n",
    "        # self.dense_1 = nn.Linear(embedding_dim * 2 + 2, 128)\n",
    "        # self.output = nn.Linear(128, 2)\n",
    "        self.fc2 = nn.Linear(embedding_dim * 2 + 2, 2)\n",
    "        \n",
    "    def forward(self, x, aux_input):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        lang_output = self.fc(x[:, -1, :])\n",
    "        # final_output = self.fc_with_aux(torch.cat((aux_input, x[:, -1, :]), dim=1))\n",
    "        \n",
    "        # out, _ = self.lstm_1(x)\n",
    "        # out, _ = self.lstm_2(out)\n",
    "        # lang_output = torch.sigmoid(self.language_model_out(out[:, -1, :]))\n",
    "        x_combined = torch.cat((aux_input, x[:, -1, :]), dim=1)\n",
    "        x_combined = self.batch_norm(x_combined)\n",
    "        # x_combined = torch.relu(self.dense_1(x_combined))\n",
    "        # final_output = torch.sigmoid(self.output(x_combined))\n",
    "        final_output = self.fc2(x_combined)\n",
    "        return final_output, lang_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_suite_name(g: str) -> str:\n",
    "    c = g.split('-')\n",
    "    if c[0] == \"amd\" or c[0] == \"nvidia\":\n",
    "        return c[0].upper() + \" SDK\"\n",
    "    if c[0] == \"npb\" or c[0] == \"shoc\":\n",
    "        return c[0].upper()\n",
    "    elif c[0] == \"parboil\" or c[0] == \"polybench\" or c[0] == \"rodinia\":\n",
    "        return c[0].capitalize()\n",
    "    else:\n",
    "        raise LookupError\n",
    "\n",
    "def escape_benchmark_name(g: str) -> str:\n",
    "    c = g.split('-')\n",
    "    return escape_suite_name(c[0]).split()[0] + \".\" + c[-2]\n",
    "\n",
    "def eval_model(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    pred_list, label_list =  [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            sequences, aux_input, labels = [b.to(device) for b in batch]\n",
    "            outputs, lang_outputs = model(sequences, aux_input)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            pred_list.extend(preds.tolist())\n",
    "            label_list.extend(batch[2].tolist())\n",
    "            \n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return accuracy, pred_list, label_list\n",
    "\n",
    "def train_model(model, train_loader, test_loader,  criterion, optimizer, num_epochs, model_path):\n",
    "    # 模型训练\n",
    "    pre_eval_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        model.train()\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=False)\n",
    "        for idx, batch in enumerate(progress_bar):\n",
    "            sequences, aux_input, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, lang_outputs = model(sequences, aux_input)\n",
    "\n",
    "            # 计算loss值 由output和lang_outputs与label计算CrossEntropyLoss\n",
    "            loss = criterion(outputs, labels) + 0.2 * criterion(lang_outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=epoch_loss / (idx + 1))\n",
    "                    \n",
    "        accuracy = correct / len(train_loader.dataset)\n",
    "        eval_acc,_ , _ = eval_model(model, test_loader)\n",
    "        print(f\"epoch {epoch+1}/{num_epochs}, loss: {epoch_loss:.4f}, train_acc: {accuracy:.4f}, eval_acc: {eval_acc:.4f}\")\n",
    "\n",
    "        if eval_acc > pre_eval_acc:\n",
    "            pre_eval_acc = eval_acc\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, weights_only=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Read data from data/devmap_data\n",
      "--- Preparing to read 680 input files from folder data/devmap_data/kernels_ir/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding files:  82%|████████▏ | 555/680 [00:40<00:09, 13.57it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Input file not found: data/devmap_data/kernels_ir/parboil-0.2-bfs-BFS_kernel_1M.ll",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, platform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(platform_list):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# 读取数据集\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     sequences, aux_in, y, embeddings, df \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplatform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     aux_in_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(aux_in, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      9\u001b[0m     y_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(data_path, platform)\u001b[0m\n\u001b[1;32m     37\u001b[0m         seqs\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;28mint\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m encode_ir])\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput file not found: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m file)\n\u001b[1;32m     41\u001b[0m maxlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(seq_lengths)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of benchmark  : \u001b[39m\u001b[38;5;132;01m{:>5}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(num_files))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Input file not found: data/devmap_data/kernels_ir/parboil-0.2-bfs-BFS_kernel_1M.ll"
     ]
    }
   ],
   "source": [
    "# platform_list = [\"amd\", \"nvidia\"]\n",
    "platform_list = [\"amd\", \"nvidia\"]\n",
    "\n",
    "data = []\n",
    "for i, platform in enumerate(platform_list):\n",
    "    # 读取数据集\n",
    "    sequences, aux_in, y, embeddings, df = load_data(data_folder, platform)\n",
    "    aux_in_tensor = torch.tensor(aux_in, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "    # 使用 F.normalize 进行 L2 归一化\n",
    "    # embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "    # embedding_matrix_normalized = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=204)\n",
    "    for j, (train_index, test_index) in enumerate(kf.split(sequences, y)):\n",
    "        print('--- Cross validation step [', j, '/ 10 ]')\n",
    "\n",
    "        model_basename = 'lstm'\n",
    "        model_path = os.path.join(out_folder, f\"models/{model_basename}-{platform}-{j}.pth\")\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        log_dir = os.path.join(out_folder, \"logs\")\n",
    "\n",
    "        # 读取数据集\n",
    "        train_data = DevMapDataset(sequences[train_index], aux_in_tensor[train_index], y_tensor[train_index])\n",
    "        test_data = DevMapDataset(sequences[test_index], aux_in_tensor[test_index], y_tensor[test_index])\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "        \n",
    "        if not os.path.exists(model_path):\n",
    "            # 创建模型\n",
    "            model = DevMapLSTM(200, 3, 0.5).to(device)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "            print('--- Training model... ')\n",
    "            train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, model_path)\n",
    "            \n",
    "        else:\n",
    "            # 读取模型权重文件\n",
    "            model = DevMapLSTM(200, 3, 0.5)\n",
    "            model.load_state_dict(torch.load(model_path, weights_only=False))\n",
    "            model = model.to(device)\n",
    "            print(\"Found trained model in\", model_path, \", skipping...\")\n",
    "            \n",
    "        # 模型预测\n",
    "        eval_acc, pred_list, label_list = eval_model(model, test_loader)\n",
    "        print(f'--- Evaluate Accuracy {eval_acc:.4f}')\n",
    "        benchmarks = df['benchmark'].values[test_index]\n",
    "        correct = np.array(pred_list) == np.array(label_list)\n",
    "        zero_r_dev = \"runtime_cpu\" if platform == \"amd\" else \"runtime_gpu\"\n",
    "        zer_r_runtimes = df[zero_r_dev].values[test_index]\n",
    "        runtimes = df[['runtime_cpu', 'runtime_gpu']].values[test_index]\n",
    "        p_runtimes = [r[p_] for p_, r in zip(np.array(pred_list, dtype=int), runtimes)]\n",
    "        p_speedup = zer_r_runtimes / p_runtimes\n",
    "\n",
    "        assert len(benchmarks) == len(label_list) == len(correct) == len(pred_list) == len(p_speedup)\n",
    "\n",
    "        for benchmark_, o_, p_, correct_, p_speedup_ in zip(benchmarks, label_list, pred_list, correct, p_speedup):\n",
    "            data.append({\n",
    "                \"Model\": model_basename,\n",
    "                \"Platform\": platform2str[platform],\n",
    "                'Benchmark': escape_benchmark_name(benchmark_),\n",
    "                'Benchmark Suite': escape_suite_name(benchmark_),\n",
    "                \"Oracle Mapping\": int(o_),\n",
    "                \"Predicted Mapping\": int(p_),\n",
    "                \"Correct?\": bool(correct_),\n",
    "                \"Speedup\": float(p_speedup_),\n",
    "            })\n",
    "            \n",
    "result =  pd.DataFrame(\n",
    "    data, index=range(1, len(data) + 1), columns=[\n",
    "        \"Model\",\n",
    "        \"Platform\",\n",
    "        \"Benchmark\",\n",
    "        \"Benchmark Suite\",\n",
    "        \"Oracle Mapping\",\n",
    "        \"Predicted Mapping\",\n",
    "        \"Correct?\",\n",
    "        \"Speedup\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_pred_vals = [58.823529, 56.911765]\n",
    "static_pred_mean = 57.867647\n",
    "static_sp_vals = [1.0, 1.0]\n",
    "static_sp_mean = 1.0\n",
    "grewe_pred_vals = [73.382353, 72.941176]\n",
    "grewe_pred_mean = 73.161765\n",
    "grewe_sp_vals = [2.905822, 1.264801]\n",
    "grewe_sp_mean = 2.085312\n",
    "deeptune_pred_vals = [83.676471, 80.294118]\n",
    "deeptune_pred_mean = 81.985294\n",
    "deeptune_sp_vals = [3.335612, 1.412222]\n",
    "deeptune_sp_mean = 2.373917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction results\n",
      "                                 Correct?    Speedup\n",
      "Platform        Benchmark Suite                     \n",
      "AMD Tahiti 7970 AMD SDK          0.562500   0.910442\n",
      "                NPB              0.789374   2.500331\n",
      "                NVIDIA SDK       0.833333   2.931322\n",
      "                Parboil          0.500000   8.551945\n",
      "                Polybench        0.925926  14.120785\n",
      "                Rodinia          0.516129   3.820326\n",
      "                SHOC             0.791667   1.465588\n",
      "NVIDIA GTX 970  AMD SDK          0.937500   1.000000\n",
      "                NPB              0.707780   1.064432\n",
      "                NVIDIA SDK       0.416667   0.956810\n",
      "                Parboil          0.375000   1.143786\n",
      "                Polybench        0.666667   1.084676\n",
      "                Rodinia          0.451613   1.021456\n",
      "                SHOC             0.479167   1.448835\n",
      "\n",
      "--- Prediction results (summarized)\n",
      "                 Correct?   Speedup\n",
      "Platform                           \n",
      "AMD Tahiti 7970  0.774290  2.998314\n",
      "NVIDIA GTX 970   0.674141  1.088315\n"
     ]
    }
   ],
   "source": [
    "print('\\n--- Prediction results')\n",
    "print(result.groupby(['Platform', 'Benchmark Suite'])[['Correct?', 'Speedup']].mean())\n",
    "print('\\n--- Prediction results (summarized)')\n",
    "print(result.groupby(['Platform'])[['Correct?', 'Speedup']].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model comparison: prediction accuracy\n",
      "\n",
      "                  Static mapping  Grewe et al.   DeepTune  DeepTuneInst2Vec\n",
      "AMD Tahiti 7970       58.823529     73.382353  83.676471         77.428999\n",
      "NVIDIA GTX 970        56.911765     72.941176  80.294118         67.414051\n",
      "Average               57.867647     73.161765  81.985294         72.421525\n"
     ]
    }
   ],
   "source": [
    "# Model comparison: prediction accuracy\n",
    "print('\\n--- Model comparison: prediction accuracy')\n",
    "d = list()\n",
    "d.append(np.append(static_pred_vals, static_pred_mean))\n",
    "d.append(np.append(grewe_pred_vals, grewe_pred_mean))\n",
    "d.append(np.append(deeptune_pred_vals, deeptune_pred_mean))\n",
    "d.append(np.append(result.groupby(['Platform'])['Correct?'].mean().values * 100,\n",
    "                    result['Correct?'].mean() * 100))\n",
    "d = np.array(d).T\n",
    "print('\\n', pd.DataFrame(d, columns=['Static mapping', 'Grewe et al.', 'DeepTune', 'DeepTuneInst2Vec'],\n",
    "                             index=['AMD Tahiti 7970', 'NVIDIA GTX 970', 'Average']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model comparison: speedups\n",
      "\n",
      "                  Static mapping  Grewe et al.  DeepTune  DeepTuneInst2Vec\n",
      "AMD Tahiti 7970             1.0      2.905822  3.335612          2.998314\n",
      "NVIDIA GTX 970              1.0      1.264801  1.412222          1.088315\n",
      "Average                     1.0      2.085312  2.373917          2.043315\n"
     ]
    }
   ],
   "source": [
    "# Model comparison: speedups\n",
    "print('\\n--- Model comparison: speedups')\n",
    "d = list()\n",
    "d.append(np.append(static_sp_vals, static_sp_mean))\n",
    "d.append(np.append(grewe_sp_vals, grewe_sp_mean))\n",
    "d.append(np.append(deeptune_sp_vals, deeptune_sp_mean))\n",
    "d.append(np.append(result.groupby(['Platform'])['Speedup'].mean().values,\n",
    "                    result['Speedup'].mean()))\n",
    "d = np.array(d).T\n",
    "print('\\n', pd.DataFrame(d, columns=['Static mapping', 'Grewe et al.', 'DeepTune', 'DeepTuneInst2Vec'],\n",
    "                            index=['AMD Tahiti 7970', 'NVIDIA GTX 970', 'Average']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
