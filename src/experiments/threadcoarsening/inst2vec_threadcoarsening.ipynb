{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from src.utils.system import read_ir_from_file\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from src.observation.inst2vec import Inst2vecEncoder\n",
    "from scipy.stats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/threadcoarsening_data'\n",
    "platform = 'all'\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "dense_layer_size = 32\n",
    "print_summary = False\n",
    "out_folder = 'output/inst2vec_for_threadcoarsening'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cfs = [1, 2, 4, 8, 16, 32]  # thread coarsening factors\n",
    "platform_list = [\"Cypress\", \"Tahiti\", \"Fermi\", \"Kepler\"]\n",
    "_FLAG_TO_DEVICE_NAME = {\n",
    "    'Cypress': 'AMD Radeon HD 5900',\n",
    "    'Tahiti': 'AMD Tahiti 7970',\n",
    "    'Fermi': 'NVIDIA GTX 480',\n",
    "    'Kepler': 'NVIDIA Tesla K20c'\n",
    "}\n",
    "\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "\n",
    "assert platform in [\"all\", \"Cypress\", \"Tahiti\", \"Fermi\", \"Kepler\"], \\\n",
    "        'Choose device among: all, Cypress, Tahiti, Fermi, Kepler'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_runtime(df, kernel, cf, platform):\n",
    "    filter1 = df[\"kernel\"] == kernel\n",
    "    filter2 = df[\"cf\"] == cf\n",
    "    return df.where(filter1 & filter2)[\"runtime_\" + platform].dropna()\n",
    "\n",
    "\n",
    "def load_data(data_folder):\n",
    "    oracle_file = os.path.join(data_folder + \"/pact-2014-oracles.csv\")\n",
    "    oracles = pd.read_csv(oracle_file)\n",
    "\n",
    "    runtimes_file = os.path.join(data_folder + \"/pact-2014-runtimes.csv\")\n",
    "    df = pd.read_csv(runtimes_file)\n",
    "\n",
    "    cfs = np.array([1, 2, 4, 8, 16, 32])\n",
    "    kernel_freq = df[\"kernel\"].value_counts().sort_index().reset_index()\n",
    "\n",
    "    inferencetime = []\n",
    "    llfiles = pd.read_csv(data_folder + \"/all.txt\", sep=\"\\s+\")\n",
    "    fileNum = llfiles[\"FileNum\"]\n",
    "    filesname = llfiles[\"ProgramName\"]\n",
    "    oracles[\"kernel_path\"] = str(\"./\") + oracles[\"kernel\"] + str(\".ll\")\n",
    "    resultant_data = pd.DataFrame()\n",
    "    for i, platform in enumerate(platform_list):\n",
    "        data = pd.merge(\n",
    "            llfiles, oracles, left_on=\"ProgramName\", right_on=\"kernel_path\"\n",
    "        )\n",
    "        data[\"cf\"] = data[\"cf_\" + platform]\n",
    "        data[\"device\"] = i + 1\n",
    "        resultant_data = pd.concat([resultant_data, data])\n",
    "\n",
    "    resultant_data = pd.get_dummies(resultant_data, columns=[\"device\"])\n",
    "    resultant_data.reset_index(inplace=True)\n",
    "\n",
    "    encoder = Inst2vecEncoder()  # inst2vec 编码器\n",
    "    unk_idx = encoder.unknown_vocab_element    \n",
    "    print('--- Preparing to read', len(resultant_data), 'input files from folder', data_folder + '/kernels_ir/')\n",
    "    seqs = list()\n",
    "    seq_lengths = list()\n",
    "    num_unks = 0\n",
    "\n",
    "    # 遍历文件\n",
    "    for file in tqdm(resultant_data[\"kernel\"], desc='Encoding files'):\n",
    "        # print(file)\n",
    "        file = data_folder + '/kernels_ir/' + file + str(\".ll\")\n",
    "        if os.path.exists(file):\n",
    "            ir = encoder.preprocess(file)\n",
    "            encode_ir = encoder.encode(ir)  # inst2vec编码\n",
    "            seq_lengths.append(len(encode_ir))\n",
    "            num_unks += encode_ir.count(str(unk_idx))\n",
    "            seqs.append([int(s) for s in encode_ir])\n",
    "        else:\n",
    "            raise FileNotFoundError('Input file not found: ' + file)\n",
    "        \n",
    "    maxlen = max(seq_lengths)\n",
    "    print('Number of benchmark  : {:>5}'.format(len(resultant_data)))\n",
    "    print('Shortest sequence    : {:>5}'.format(min(seq_lengths)))\n",
    "    print('Longest sequence     : {:>5}'.format(maxlen))\n",
    "    print('Mean sequence length : {:>5} (rounded down)'.format(math.floor(np.mean(seq_lengths))))\n",
    "    print('Number of \\'UNK\\'      : {:>5}'.format(num_unks))\n",
    "    print('Percentage of \\'UNK\\'  : {:>8.4} (% among all stmts)'.format((num_unks*100)/sum(seq_lengths)))\n",
    "    print('\\'UNK\\' index          : {:>5}'.format(unk_idx))\n",
    "\n",
    "    # Padding logic\n",
    "    padded_sequences = []\n",
    "    for seq in seqs:\n",
    "        if len(seq) < maxlen:\n",
    "            # Pad sequence if it is shorter than maxlen\n",
    "            seq = seq + [unk_idx] * (maxlen - len(seq))\n",
    "        padded_sequences.append(seq)\n",
    "\n",
    "    # Convert to np.array\n",
    "    encoded = np.array(padded_sequences)\n",
    "\n",
    "    targetLabel = resultant_data[\"cf\"]\n",
    "    data = resultant_data\n",
    "    data = data.drop(\n",
    "        columns=[\n",
    "            \"index\",\n",
    "            \"FileNum\",\n",
    "            \"ProgramName\",\n",
    "            \"kernel\",\n",
    "            \"cf_Fermi\",\n",
    "            \"runtime_Fermi\",\n",
    "            \"cf_Kepler\",\n",
    "            \"runtime_Kepler\",\n",
    "            \"cf_Cypress\",\n",
    "            \"runtime_Cypress\",\n",
    "            \"cf_Tahiti\",\n",
    "            \"runtime_Tahiti\",\n",
    "            \"kernel_path\",\n",
    "            \"cf\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    assert len(encoded) == len(data) == len(targetLabel)\n",
    "\n",
    "    return encoded, data, targetLabel, encoder.embeddings, df, oracles\n",
    "\n",
    "class ThreadCoaDataset(Dataset):\n",
    "    def __init__(self, encoded, data, targetLabel, embeddings):\n",
    "        super().__init__()\n",
    "        self.sequences = encoded\n",
    "        self.dev = data\n",
    "        self.y = targetLabel\n",
    "        self.embeddings = embeddings\n",
    "        self.embedding_input = self.embeddings[self.sequences]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        seqs = self.embedding_input[index]\n",
    "        dev = self.dev[index]\n",
    "        label = self.y[index]\n",
    "        return seqs, dev, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络结构\n",
    "class ThreadCoaLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, dense_layer_size):\n",
    "        super(ThreadCoaLSTM, self).__init__()\n",
    "        self.lstm_1 = nn.LSTM(embedding_dim, embedding_dim, batch_first=True)\n",
    "        self.lstm_2 = nn.LSTM(embedding_dim, embedding_dim, batch_first=True)\n",
    "        self.language_model_out = nn.Linear(embedding_dim, 6)\n",
    "        self.batch_norm = nn.BatchNorm1d(embedding_dim + 4)\n",
    "        self.dense_1 = nn.Linear(embedding_dim + 4, dense_layer_size)\n",
    "        self.output = nn.Linear(dense_layer_size, 6)\n",
    "        \n",
    "    def forward(self, x, device_input):\n",
    "        out, _ = self.lstm_1(x)\n",
    "        out, _ = self.lstm_2(out)\n",
    "        lang_output = torch.sigmoid(self.language_model_out(out[:, -1, :]))\n",
    "        x_combined = torch.cat((device_input, out[:, -1, :]), dim=1)\n",
    "        x_combined = self.batch_norm(x_combined)\n",
    "        x_combined = torch.relu(self.dense_1(x_combined))\n",
    "        final_output = torch.sigmoid(self.output(x_combined))\n",
    "        return final_output, lang_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    pred_list, label_list =  [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            sequences, aux_input, labels = [b.to(device) for b in batch]\n",
    "            outputs, lang_outputs = model(sequences, aux_input)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            pred_list.extend(preds.tolist())\n",
    "            label_list.extend(batch[2].tolist())\n",
    "            \n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return accuracy, pred_list, label_list\n",
    "\n",
    "def train_model(model, train_loader, test_loader,  criterion, optimizer, num_epochs, model_path):\n",
    "    # 模型训练\n",
    "    pre_eval_acc = -1\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            sequences, aux_input, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, lang_outputs = model(sequences, aux_input)\n",
    "\n",
    "            # 计算loss值 由output和lang_outputs与label计算CrossEntropyLoss\n",
    "            loss = criterion(outputs, labels) + 0.2 * criterion(lang_outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "                    \n",
    "        accuracy = correct / len(train_loader.dataset)\n",
    "        eval_acc,_ , _ = eval_model(model, test_loader)\n",
    "        # print(f\"epoch {epoch+1}/{num_epochs}, loss: {epoch_loss:.4f}, train_acc: {accuracy:.4f}, eval_acc: {eval_acc:.4f}\")\n",
    "\n",
    "        if eval_acc > pre_eval_acc:\n",
    "            pre_eval_acc = eval_acc\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, weights_only=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# platform_list = [\"amd\", \"nvidia\"]\n",
    "sequences, data, targetLabel, embeddings, df, oracles = load_data(data_folder)\n",
    "device_onehot_tensor = torch.tensor(data.values, dtype=torch.float32)\n",
    "y_tensor = torch.argmax(torch.tensor(pd.get_dummies(targetLabel).values, dtype=torch.float32), dim=1)\n",
    "cfs = np.array([1.0, 2.0, 4.0, 8.0, 16.0, 32.0])\n",
    "kernel_freq = df[\"kernel\"].value_counts().sort_index().reset_index()\n",
    "\n",
    "\n",
    "# 使用 F.normalize 进行 L2 归一化\n",
    "embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "embedding_matrix_normalized = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "data = []\n",
    "kf = KFold(n_splits=len(targetLabel), shuffle=False)\n",
    "for j, (train_index, test_index) in enumerate(kf.split(sequences, targetLabel)):\n",
    "    print('--- Cross validation step [', j, '/ ',len(targetLabel),' ]')\n",
    "    kernel = sorted(set(df[\"kernel\"]))[test_index[0] % 17]\n",
    "\n",
    "    model_basename = 'lstm'\n",
    "    model_path = os.path.join(out_folder, f\"models/{model_basename}-{platform}-{j}.pth\")\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    log_dir = os.path.join(out_folder, \"logs\")\n",
    "    \n",
    "    # 读取数据集\n",
    "    train_data = ThreadCoaDataset(sequences[train_index], device_onehot_tensor[train_index], y_tensor[train_index], embedding_matrix_normalized)\n",
    "    test_data = ThreadCoaDataset(sequences[test_index], device_onehot_tensor[test_index], y_tensor[test_index], embedding_matrix_normalized)\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        # 创建模型\n",
    "        model = ThreadCoaLSTM(embedding_dim=embedding_matrix_normalized.shape[1], dense_layer_size=dense_layer_size).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        print('--- Training model... ')\n",
    "        train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, model_path)\n",
    "        \n",
    "    else:\n",
    "        # 读取模型权重文件\n",
    "        model = ThreadCoaLSTM(embedding_dim=embedding_matrix_normalized.shape[1], dense_layer_size=dense_layer_size).to(device)\n",
    "        model.load_state_dict(torch.load(model_path, weights_only=False))\n",
    "        model = model.to(device)\n",
    "        print(\"Found trained model in\", model_path, \", skipping...\")\n",
    "\n",
    "    # 模型预测\n",
    "    accuracy, pred_list, label_list = eval_model(model, test_loader)\n",
    "    # accuracy, pred_list, label_list\n",
    "    prediction = cfs[pred_list[0]]\n",
    "\n",
    "    if device_onehot_tensor[test_index].tolist()[0][0] == 1:\n",
    "        platform = platform_list[0]\n",
    "    elif device_onehot_tensor[test_index].tolist()[0][1] == 1:\n",
    "        platform = platform_list[1]\n",
    "    elif device_onehot_tensor[test_index].tolist()[0][2] == 1:\n",
    "        platform = platform_list[2]\n",
    "    elif device_onehot_tensor[test_index].tolist()[0][3] == 1:\n",
    "        platform = platform_list[3]\n",
    "\n",
    "    oracle_runtimes = np.array([float(x) for x in oracles[\"runtime_\" + platform]])\n",
    "    oracle = targetLabel[test_index[0]]\n",
    "    print(oracle == prediction)\n",
    "\n",
    "    rt_baseline = float(find_runtime(df, kernel, 1, platform))\n",
    "    rt_pred = float(find_runtime(df, kernel, prediction, platform))\n",
    "    rt_oracle = float(oracle_runtimes[test_index[0] % 17])\n",
    "    data.append(\n",
    "        {\n",
    "            \"Model\": \"IR2vec\",\n",
    "            \"Platform\": _FLAG_TO_DEVICE_NAME[platform],\n",
    "            \"Kernel\": kernel,\n",
    "            \"Oracle-CF\": oracle,\n",
    "            \"Predicted-CF\": prediction,\n",
    "            \"Speedup\": rt_baseline / rt_pred,\n",
    "            \"Oracle\": rt_oracle / rt_pred,\n",
    "            \"OracleSpeedUp\": rt_baseline / rt_oracle,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results from other works\n",
    "\n",
    "The accuracies and speedups are taken from the results quoted by NCC in their work for the purpose of comparison. For detailed analysis (discussed later), we run these models and the obtained results are stored as pickle files in ./data/prior_art_results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "magni_sp_vals = [1.21, 1.01, 0.86, 0.94]\n",
    "magni_sp_mean = [1.005]\n",
    "deeptune_sp_vals = [1.10, 1.05, 1.10, 0.99]\n",
    "deeptune_sp_mean = [1.06]\n",
    "deeptuneTL_sp_vals = [1.17, 1.23, 1.14, 0.93]\n",
    "deeptuneTL_sp_mean = [1.1175]\n",
    "ncc_sp_vals = [1.29, 1.07, 0.97, 1.01]\n",
    "ncc_sp_mean = [1.086]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR2Vec Flow-Aware Vs. Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Speedup Matrix: IR2Vec Vs. others\n",
      "\n",
      "                    Magni et al.  DeepTune  DeepTune-TL    NCC  Inst2Vec\n",
      "AMD Radeon HD 5900         1.210      1.10       1.1700  1.290  1.228449\n",
      "AMD Tahiti 7970            1.010      1.05       1.2300  1.070  1.252760\n",
      "NVIDIA GTX 480             0.860      1.10       1.1400  0.970  1.109817\n",
      "NVIDIA Tesla K20c          0.940      0.99       0.9300  1.010  1.154982\n",
      "Average                    1.005      1.06       1.1175  1.086  1.186502\n"
     ]
    }
   ],
   "source": [
    "ir2vec = pd.DataFrame(\n",
    "    data,\n",
    "    columns=[\n",
    "        \"Model\",\n",
    "        \"Platform\",\n",
    "        \"Kernel\",\n",
    "        \"Oracle-CF\",\n",
    "        \"Predicted-CF\",\n",
    "        \"Speedup\",\n",
    "        \"Oracle\",\n",
    "        \"OracleSpeedUp\",\n",
    "    ],\n",
    ")\n",
    "print(\"\\nSpeedup Matrix: IR2Vec Vs. others\\n\")\n",
    "ir2vec_sp_vals = ir2vec.groupby([\"Platform\"])[\"Speedup\"].mean().values\n",
    "ir2vec_sp_mean = ir2vec_sp_vals.mean()\n",
    "sp_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Magni et al.\": magni_sp_vals + magni_sp_mean,\n",
    "        \"DeepTune\": deeptune_sp_vals + deeptune_sp_mean,\n",
    "        \"DeepTune-TL\": deeptuneTL_sp_vals + deeptuneTL_sp_mean,\n",
    "        \"NCC\": ncc_sp_vals + ncc_sp_mean,\n",
    "        \"Inst2Vec\": list(ir2vec_sp_vals) + [ir2vec_sp_mean],\n",
    "    },\n",
    "    index=[\n",
    "        \"AMD Radeon HD 5900\",\n",
    "        \"AMD Tahiti 7970\",\n",
    "        \"NVIDIA GTX 480\",\n",
    "        \"NVIDIA Tesla K20c\",\n",
    "        \"Average\",\n",
    "    ],\n",
    ")\n",
    "print(sp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other related observations\n",
    "For the comparison, we use the results obtained on training the earlier works  \n",
    "## Speedup comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "magni_res = pd.read_pickle(data_folder + \"/prior_art_results/magni_tf.results\")\n",
    "deeptune_res = pd.read_pickle(data_folder + \"/prior_art_results/deeptune_tf.results\")\n",
    "deeptune_tl_res = pd.read_pickle(data_folder + \"/prior_art_results/deeptune_tl_tf.results\")\n",
    "ncc_res = pd.read_pickle(data_folder + \"/prior_art_results/ncc_fix_tf.results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric mean of Magni et al. 0.86x\n",
      "Geometric mean of DeepTune 1.00x\n",
      "Geometric mean of Inst2Vec 1.15x\n"
     ]
    }
   ],
   "source": [
    "magni_geomean = gmean(magni_res[\"Speedup\"].values)\n",
    "deeptune_geomean = gmean(deeptune_res[\"Speedup\"].values)\n",
    "deeptune_tl_geomean = gmean(deeptune_tl_res[\"Speedup\"].values)\n",
    "ncc_geomean = gmean(ncc_res[\"Speedup\"].values)\n",
    "inst2vec_geomean = gmean(ir2vec[\"Speedup\"].values)\n",
    "\n",
    "\n",
    "print(f\"Geometric mean of Magni et al. {magni_geomean:.2f}x\")\n",
    "print(f\"Geometric mean of DeepTune {deeptune_geomean:.2f}x\")\n",
    "print(f\"Geometric mean of Inst2Vec {inst2vec_geomean:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcSpeedup(platform):\n",
    "    magni_geomean = gmean(\n",
    "        magni_res[magni_res[\"Platform\"] == platform][\"Speedup\"].values\n",
    "    )\n",
    "    deeptune_geomean = gmean(\n",
    "        deeptune_res[deeptune_res[\"Platform\"] == platform][\"Speedup\"].values\n",
    "    )\n",
    "    deeptune_tl_geomean = gmean(\n",
    "        deeptune_tl_res[deeptune_tl_res[\"Platform\"] == platform][\"Speedup\"].values\n",
    "    )\n",
    "    ncc_geomean = gmean(ncc_res[ncc_res[\"Platform\"] == platform][\"Speedup\"].values)\n",
    "    ir2vec_sym_geomean = gmean(\n",
    "        ir2vec[ir2vec[\"Platform\"] == platform][\"Speedup\"].values\n",
    "    )\n",
    "\n",
    "    print(f\"Geometric mean of Magni et al. {magni_geomean:.2f}x\")\n",
    "    print(f\"Geometric mean of DeepTune {deeptune_geomean:.2f}x\")\n",
    "    print(f\"Geometric mean of DeepTune-TL {deeptune_tl_geomean:.2f}x\")\n",
    "    print(f\"Geometric mean of Inst2Vec {ir2vec_sym_geomean:.2f}x\")\n",
    "\n",
    "    return (\n",
    "        round(magni_geomean, 2),\n",
    "        round(deeptune_geomean, 2),\n",
    "        round(deeptune_tl_geomean, 2),\n",
    "        round(ir2vec_sym_geomean, 3),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric mean of Magni et al. 0.94x\n",
      "Geometric mean of DeepTune 1.14x\n",
      "Geometric mean of DeepTune-TL 1.14x\n",
      "Geometric mean of Inst2Vec 1.19x\n"
     ]
    }
   ],
   "source": [
    "rad_magni, rad_dt, rad_dtTL, rad_inst2vec = calcSpeedup(\n",
    "    \"AMD Radeon HD 5900\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric mean of Magni et al. 0.98x\n",
      "Geometric mean of DeepTune 0.95x\n",
      "Geometric mean of DeepTune-TL 0.90x\n",
      "Geometric mean of Inst2Vec 1.20x\n"
     ]
    }
   ],
   "source": [
    "tah_magni, tah_dt, tah_dtTL, tah_inst2vec = calcSpeedup(\n",
    "    \"AMD Tahiti 7970\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric mean of Magni et al. 0.81x\n",
      "Geometric mean of DeepTune 0.94x\n",
      "Geometric mean of DeepTune-TL 0.99x\n",
      "Geometric mean of Inst2Vec 1.09x\n"
     ]
    }
   ],
   "source": [
    "gtx_magni, gtx_dt, gtx_dtTL, gtx_inst2vec = calcSpeedup(\n",
    "    \"NVIDIA GTX 480\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric mean of Magni et al. 0.74x\n",
      "Geometric mean of DeepTune 0.98x\n",
      "Geometric mean of DeepTune-TL 1.01x\n",
      "Geometric mean of Inst2Vec 1.13x\n"
     ]
    }
   ],
   "source": [
    "tes_magni, tes_dt, tes_dtTL, tes_inst2vec = calcSpeedup(\n",
    "    \"NVIDIA Tesla K20c\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
