{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2036866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(\"/home/xucong24/Compiler\")\n",
    "\n",
    "from transformers import (\n",
    "    EncoderDecoderConfig,\n",
    "    EncoderDecoderModel,\n",
    "    AutoConfig,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d951b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 126, 127, 125)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.model import (\n",
    "    Inst2VecTokenizer,\n",
    "    OptiSeqTokenizer\n",
    ")\n",
    "\n",
    "# load tokenizer\n",
    "encoder_tokenizer_path = \"/home/xucong24/Compiler/checkpoints/Inst2VecTokenizer\"\n",
    "decoder_tokenizer_path = \"/home/xucong24/Compiler/checkpoints/OptiSeqTokenizer\"\n",
    "\n",
    "encoder_tokenizer = Inst2VecTokenizer.from_pretrained(encoder_tokenizer_path)\n",
    "decoder_tokenizer = OptiSeqTokenizer.from_pretrained(decoder_tokenizer_path)\n",
    "\n",
    "decoder_tokenizer.vocab_size, decoder_tokenizer.bos_token_id, decoder_tokenizer.eos_token_id, decoder_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc5a82a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT2LMHeadModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m decoder_config \u001b[38;5;241m=\u001b[39m GPT2Config(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[1;32m     26\u001b[0m encoder \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/xucong24/Compiler/checkpoints/modernbert_poj104_mlm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m decoder \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2LMHeadModel\u001b[49m(decoder_config)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GPT2LMHeadModel' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    GPT2Config,\n",
    "    AutoModel\n",
    ")\n",
    "\n",
    "config = {\n",
    "    \"n_embd\": 768,\n",
    "    \"n_head\": 12,\n",
    "    \"n_layer\": 6,\n",
    "    \"bos_token_id\": 126,\n",
    "    \"eos_token_id\": 127,\n",
    "    \"vocab_size\": 128,\n",
    "    \"add_cross_attention\": True,\n",
    "    # \"architectures\": [\n",
    "    #     \"GPT2LMHeadModel\"\n",
    "    # ],\n",
    "    # \"task_specific_params\": {\n",
    "    #     \"text-generation\": {\n",
    "    #     \"do_sample\": True,\n",
    "    #     \"max_length\": 50\n",
    "    #     }\n",
    "    # },\n",
    "}\n",
    "\n",
    "decoder_config = GPT2Config(**config)\n",
    "encoder = AutoModel.from_pretrained(\"/home/xucong24/Compiler/checkpoints/modernbert_poj104_mlm\")\n",
    "decoder = GPT2LMHeadModel(decoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "50c5c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    EncoderDecoderConfig,\n",
    "    AutoConfig,\n",
    "    PretrainedConfig\n",
    ")\n",
    "\n",
    "class PassformerConfig(EncoderDecoderConfig):\n",
    "\n",
    "    model_type = \"passformer\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        fusion_method: str = \"add\",  # \"concat\", \"add\", \"cross_attention\"\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fusion_method = fusion_method\n",
    "        self.autophase_dim = 56\n",
    "        self.decoder_start_token_id = 126\n",
    "        self.pad_token_id = 125\n",
    "        self.vocab_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1227a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from typing import Optional, Tuple, Union, List\n",
    "from transformers import (\n",
    "    PreTrainedModel\n",
    ")\n",
    "\n",
    "class AutophaseProjection(nn.Module):\n",
    "    \"\"\"Project Autophase features to encoder/decoder hidden dimension.\"\"\"\n",
    "    \n",
    "    def __init__(self, autophase_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        # self.proj = nn.Sequential(\n",
    "        #     nn.Linear(autophase_dim, hidden_dim),\n",
    "        #     nn.LayerNorm(hidden_dim),\n",
    "        #     nn.GELU(),\n",
    "        # )\n",
    "\n",
    "        immediate_dim = hidden_dim // 2\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(autophase_dim, immediate_dim),\n",
    "            nn.LayerNorm(immediate_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(immediate_dim, immediate_dim),\n",
    "            nn.LayerNorm(immediate_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(immediate_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, autophase: torch.Tensor) -> torch.Tensor:\n",
    "        return self.proj(autophase).unsqueeze(1)\n",
    "\n",
    "\n",
    "class AutophaseCrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention module for Autophase feature fusion.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            hidden_dim, num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        hidden_states: torch.Tensor, \n",
    "        autophase_emb: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        attn_output, _ = self.cross_attn(\n",
    "            query=hidden_states,\n",
    "            key=autophase_emb,\n",
    "            value=autophase_emb,\n",
    "        )\n",
    "        return self.norm(hidden_states + self.dropout(attn_output))\n",
    "\n",
    "# ============================================================================\n",
    "# Main Model\n",
    "# ============================================================================\n",
    "\n",
    "class PassformerModel(EncoderDecoderModel):\n",
    "    \"\"\"\n",
    "        >>> from src.model import PassformerModel, PassformerConfig, Inst2VecTokenizer, OptiSeqTokenizer\n",
    "        >>> from transformers import AutoModel, AutoConfig, GPT2LMHeadModel, GPT2Config\n",
    "        >>> \n",
    "        >>> # 步骤1: 加载 tokenizers（必需）\n",
    "        >>> encoder_tokenizer = Inst2VecTokenizer.from_pretrained(\"path/to/encoder_tokenizer\")\n",
    "        >>> decoder_tokenizer = OptiSeqTokenizer.from_pretrained(\"path/to/decoder_tokenizer\")\n",
    "        >>> \n",
    "        >>> # 步骤2: 创建 encoder 和 decoder\n",
    "        >>> encoder = AutoModel.from_pretrained(\"path/to/encoder_model\")\n",
    "        >>> decoder_config = GPT2Config(\n",
    "        ...     vocab_size=decoder_tokenizer.vocab_size,\n",
    "        ...     n_embd=768,\n",
    "        ...     n_layer=6,\n",
    "        ...     n_head=12,\n",
    "        ... )\n",
    "        >>> decoder = GPT2LMHeadModel(decoder_config)\n",
    "\n",
    "        >>> # 步骤4: 创建模型\n",
    "        >>> model = PassformerModel(encoder=encoder, decoder=decoder, fusion_method='add')\n",
    "        >>> \n",
    "        >>> # 步骤5: 使用 tokenizer 进行编码\n",
    "        >>> llvm_ir = \"define void @main() { ... }\"\n",
    "        >>> encoder_inputs = encoder_tokenizer(llvm_ir, max_length=1024, return_tensors=\"pt\")\n",
    "        >>> \n",
    "        >>> opti_seq = \"mem2reg instcombine gvn\"\n",
    "        >>> decoder_inputs = decoder_tokenizer(opti_seq, max_length=256, return_tensors=\"pt\")\n",
    "        >>> \n",
    "        >>> # 模型前向传播\n",
    "        >>> outputs = model(\n",
    "        ...     input_ids=encoder_inputs[\"input_ids\"],\n",
    "        ...     attention_mask=encoder_inputs[\"attention_mask\"],\n",
    "        ...     decoder_input_ids=decoder_inputs[\"input_ids\"],\n",
    "        ...     decoder_attention_mask=decoder_inputs[\"attention_mask\"],\n",
    "        ...     labels=decoder_inputs[\"input_ids\"],\n",
    "        ...     autophase=autophase_features,  # [batch, 56]\n",
    "        ... )\n",
    "    \"\"\"\n",
    "\n",
    "    config: PassformerConfig\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[PretrainedConfig] = None,\n",
    "        encoder: Optional[PreTrainedModel] = None,\n",
    "        decoder: Optional[PreTrainedModel] = None,\n",
    "        fusion_method: Optional[str] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        super().__init__(encoder=encoder, decoder=decoder, config=config, **kwargs)\n",
    "\n",
    "        config_dict = self.config.to_dict()\n",
    "        if fusion_method is not None:\n",
    "            config_dict['fusion_method'] = fusion_method\n",
    "        \n",
    "        self.config = PassformerConfig(**config_dict)\n",
    "\n",
    "        # 获取 hidden sizes\n",
    "        encoder_hidden_size = self.encoder.config.hidden_size\n",
    "        decoder_hidden_size = self.decoder.config.n_embd\n",
    "        \n",
    "        if self.config.fusion_method == \"concat\":\n",
    "            # 在 encoder 输出后拼接\n",
    "            self.autophase_proj = AutophaseProjection(\n",
    "                self.config.autophase_dim, encoder_hidden_size\n",
    "            )\n",
    "        elif self.config.fusion_method == \"add\":\n",
    "            # 加到 encoder 输出\n",
    "            self.autophase_proj = AutophaseProjection(\n",
    "                self.config.autophase_dim, encoder_hidden_size\n",
    "            )\n",
    "        elif self.config.fusion_method == \"cross_attention\":\n",
    "            # Cross-attention 融合\n",
    "            self.autophase_proj = AutophaseProjection(\n",
    "                self.config.autophase_dim, encoder_hidden_size\n",
    "            )\n",
    "            self.autophase_cross_attn = AutophaseCrossAttention(\n",
    "                encoder_hidden_size, \n",
    "                num_heads=8\n",
    "            )\n",
    "        elif self.config.fusion_method is not None and self.config.fusion_method != \"none\":\n",
    "            raise ValueError(f\"Unknown fusion method: {self.config.fusion_method}\")\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        autophase: Optional[torch.FloatTensor] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # 处理 Autophase 融合\n",
    "        if autophase is not None:\n",
    "            if encoder_outputs is None:\n",
    "                encoder_outputs = self.encoder(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_attentions=output_attentions,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "            \n",
    "            # 融合 autophase 特征\n",
    "            encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "            fused_hidden_states, updated_attention_mask = self._fuse_autophase(\n",
    "                encoder_hidden_states, attention_mask, autophase\n",
    "            )\n",
    "            \n",
    "            # 更新 encoder_outputs\n",
    "            encoder_outputs.last_hidden_state = fused_hidden_states\n",
    "            attention_mask = updated_attention_mask\n",
    "        \n",
    "        # 调用父类的 forward 方法\n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs,\n",
    "        )\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        inputs=None,\n",
    "        generation_config=None,\n",
    "        logits_processor=None,\n",
    "        stopping_criteria=None,\n",
    "        prefix_allowed_tokens_fn=None,\n",
    "        synced_gpus=False,\n",
    "        autophase=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate optimization sequences with Autophase support.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Encoder input token IDs [batch, seq_len]\n",
    "            autophase: Autophase features [batch, autophase_dim]\n",
    "            ... (其他标准 generate 参数)\n",
    "        \n",
    "        Returns:\n",
    "            Generated token IDs [batch, gen_len]\n",
    "        \"\"\"\n",
    "\n",
    "        if self.config.fusion_method != \"none\" and autophase is None:\n",
    "            raise ValueError(\"Autophase features are required for generation when fusion method is not 'none'\")\n",
    "            \n",
    "        # Autophase 融合\n",
    "        if autophase is not None:\n",
    "            # 获取 encoder 输入\n",
    "            if kwargs.get(\"attention_mask\") is None:\n",
    "                attention_mask = torch.ones_like(inputs)\n",
    "            else:\n",
    "                attention_mask = kwargs.get(\"attention_mask\")\n",
    "            \n",
    "            # 编码并融合 autophase\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=inputs,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "            fused_hidden_states, updated_attention_mask = self._fuse_autophase(\n",
    "                encoder_hidden_states, attention_mask, autophase\n",
    "            )\n",
    "            \n",
    "            # 更新 encoder_outputs\n",
    "            encoder_outputs.last_hidden_state = fused_hidden_states\n",
    "            kwargs[\"encoder_outputs\"] = encoder_outputs\n",
    "            kwargs[\"attention_mask\"] = updated_attention_mask\n",
    "        \n",
    "        # 调用父类的 generate 方法\n",
    "        return super().generate(\n",
    "            inputs=inputs,\n",
    "            generation_config=generation_config,\n",
    "            logits_processor=logits_processor,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "            synced_gpus=synced_gpus,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _fuse_autophase(\n",
    "        self,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        autophase: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size = encoder_hidden_states.size(0)\n",
    "        device = encoder_hidden_states.device\n",
    "\n",
    "        if autophase.dtype != torch.float32 and autophase.dtype != torch.float16:\n",
    "            autophase = autophase.float()\n",
    "        \n",
    "        \n",
    "        if self.config.fusion_method == \"concat\":\n",
    "            # Project autophase and concatenate as extra token\n",
    "            autophase_emb = self.autophase_proj(autophase)  # [batch, 1, hidden]\n",
    "            fused = torch.cat([autophase_emb, encoder_hidden_states], dim=1)\n",
    "            \n",
    "            # Update attention mask\n",
    "            autophase_mask = torch.ones(batch_size, 1, device=device, dtype=attention_mask.dtype)\n",
    "            attention_mask = torch.cat([autophase_mask, attention_mask], dim=1)\n",
    "            \n",
    "            return fused, attention_mask\n",
    "        \n",
    "        elif self.config.fusion_method == \"add\":\n",
    "            # Add autophase embedding to all positions]\n",
    "            autophase_emb = self.autophase_proj(autophase)  # [batch, 1, hidden]\n",
    "            fused = encoder_hidden_states + autophase_emb\n",
    "            return fused, attention_mask\n",
    "        \n",
    "        elif self.config.fusion_method == \"cross_attention\":\n",
    "            # Use cross-attention to fuse\n",
    "            autophase_emb = self.autophase_proj(autophase)  # [batch, 1, hidden]\n",
    "            fused = self.autophase_cross_attn(encoder_hidden_states, autophase_emb)\n",
    "            return fused, attention_mask\n",
    "        \n",
    "        else:\n",
    "            return encoder_hidden_states, attention_mask\n",
    "    \n",
    "\n",
    "model = PassformerModel(encoder=encoder, decoder=decoder, fusion_method='add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "337e8801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassformerConfig {\n",
       "  \"autophase_dim\": 56,\n",
       "  \"decoder\": {\n",
       "    \"activation_function\": \"gelu_new\",\n",
       "    \"add_cross_attention\": true,\n",
       "    \"architectures\": [\n",
       "      \"GPT2LMHeadModel\"\n",
       "    ],\n",
       "    \"attn_pdrop\": 0.1,\n",
       "    \"bos_token_id\": 126,\n",
       "    \"embd_pdrop\": 0.1,\n",
       "    \"eos_token_id\": 127,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"is_decoder\": true,\n",
       "    \"layer_norm_epsilon\": 1e-05,\n",
       "    \"model_type\": \"gpt2\",\n",
       "    \"n_embd\": 768,\n",
       "    \"n_head\": 12,\n",
       "    \"n_inner\": null,\n",
       "    \"n_layer\": 6,\n",
       "    \"n_positions\": 1024,\n",
       "    \"reorder_and_upcast_attn\": false,\n",
       "    \"resid_pdrop\": 0.1,\n",
       "    \"scale_attn_by_inverse_layer_idx\": false,\n",
       "    \"scale_attn_weights\": true,\n",
       "    \"summary_activation\": null,\n",
       "    \"summary_first_dropout\": 0.1,\n",
       "    \"summary_proj_to_labels\": true,\n",
       "    \"summary_type\": \"cls_index\",\n",
       "    \"summary_use_proj\": true,\n",
       "    \"task_specific_params\": {\n",
       "      \"text-generation\": {\n",
       "        \"do_sample\": true,\n",
       "        \"max_length\": 50\n",
       "      }\n",
       "    },\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 128\n",
       "  },\n",
       "  \"decoder_start_token_id\": 126,\n",
       "  \"encoder\": {\n",
       "    \"_name_or_path\": \"/home/xucong24/Compiler/checkpoints/modernbert_poj104_mlm\",\n",
       "    \"architectures\": [\n",
       "      \"ModernBertForMaskedLM\"\n",
       "    ],\n",
       "    \"attention_bias\": false,\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"bos_token_id\": 8566,\n",
       "    \"classifier_activation\": \"gelu\",\n",
       "    \"classifier_bias\": false,\n",
       "    \"classifier_dropout\": 0.0,\n",
       "    \"classifier_pooling\": \"mean\",\n",
       "    \"cls_token_id\": 8566,\n",
       "    \"decoder_bias\": true,\n",
       "    \"deterministic_flash_attn\": false,\n",
       "    \"dtype\": \"float32\",\n",
       "    \"embedding_dropout\": 0.0,\n",
       "    \"eos_token_id\": 8567,\n",
       "    \"global_attn_every_n_layers\": 3,\n",
       "    \"global_rope_theta\": 160000.0,\n",
       "    \"gradient_checkpointing\": false,\n",
       "    \"hidden_activation\": \"gelu\",\n",
       "    \"hidden_size\": 768,\n",
       "    \"initializer_cutoff_factor\": 2.0,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 1152,\n",
       "    \"layer_norm_eps\": 1e-05,\n",
       "    \"local_attention\": 128,\n",
       "    \"local_rope_theta\": 10000.0,\n",
       "    \"max_position_embeddings\": 8192,\n",
       "    \"mlp_bias\": false,\n",
       "    \"mlp_dropout\": 0.0,\n",
       "    \"model_type\": \"modernbert\",\n",
       "    \"norm_bias\": false,\n",
       "    \"norm_eps\": 1e-05,\n",
       "    \"num_attention_heads\": 12,\n",
       "    \"num_hidden_layers\": 22,\n",
       "    \"pad_token_id\": 8565,\n",
       "    \"position_embedding_type\": \"absolute\",\n",
       "    \"repad_logits_with_grad\": false,\n",
       "    \"sep_token_id\": 8567,\n",
       "    \"sparse_pred_ignore_index\": -100,\n",
       "    \"sparse_prediction\": false,\n",
       "    \"vocab_size\": 8569\n",
       "  },\n",
       "  \"fusion_method\": \"add\",\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"model_type\": \"passformer\",\n",
       "  \"pad_token_id\": 125,\n",
       "  \"transformers_version\": \"4.57.5\",\n",
       "  \"vocab_size\": 128\n",
       "}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7df3be41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassformerModel(\n",
       "  (encoder): ModernBertModel(\n",
       "    (embeddings): ModernBertEmbeddings(\n",
       "      (tok_embeddings): Embedding(8569, 768, padding_idx=8565)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ModernBertEncoderLayer(\n",
       "        (attn_norm): Identity()\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertRotaryEmbedding()\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1-21): 21 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertRotaryEmbedding()\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(128, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-5): 6 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D(nf=2304, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=768)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (crossattention): GPT2Attention(\n",
       "            (c_attn): Conv1D(nf=1536, nx=768)\n",
       "            (q_attn): Conv1D(nf=768, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=768)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D(nf=3072, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=3072)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=128, bias=False)\n",
       "  )\n",
       "  (autophase_proj): AutophaseProjection(\n",
       "    (proj): Sequential(\n",
       "      (0): Linear(in_features=56, out_features=384, bias=True)\n",
       "      (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (5): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (6): GELU(approximate='none')\n",
       "      (7): Dropout(p=0.1, inplace=False)\n",
       "      (8): Linear(in_features=384, out_features=768, bias=True)\n",
       "      (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38dac826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df5a650a96b49f2bee5787c1cbacea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from src.data import create_tokenize_fn\n",
    "\n",
    "dataset = load_from_disk(\"/home/xucong24/Compiler/datasets/ga_llvm_37k\")\n",
    "dataset_selected = dataset.select(range(5))\n",
    "    \n",
    "# Tokenize 数据集\n",
    "tokenize_fn = create_tokenize_fn(\n",
    "    encoder_tokenizer,\n",
    "    decoder_tokenizer,\n",
    "    128,\n",
    "    32,\n",
    "    include_autophase=True\n",
    ")\n",
    "\n",
    "remove_columns = [\n",
    "    'Benchmark', 'CpuInfo', 'IrInstructionCountO0',\n",
    "    'IrInstructionCountO3', 'IrInstructionCountOz',\n",
    "    'InstCount', 'Reward', 'LLVM_IR', 'Commandline', 'Autophase'\n",
    "]\n",
    "existing_columns = set(dataset.column_names)\n",
    "remove_columns = [col for col in remove_columns if col in existing_columns]\n",
    "\n",
    "tokenized_data = dataset_selected.map(\n",
    "    tokenize_fn,\n",
    "    batched=False,\n",
    "    remove_columns=remove_columns,\n",
    "    num_proc=32,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "tokenized_data = tokenized_data.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fda01ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[8566,    5, 8564,    8, 8564,    8, 8564, 8564, 8564, 8564,   22,   46,\n",
       "          8564,  289,  289, 8564,  289, 4062,  364,  311,  594,  293, 3032,  364,\n",
       "           295,  198,  204, 8564,  597,  600,  213, 8564,  198,  204, 8564,  364,\n",
       "           216, 8564,  231,  231, 8564, 4065,  200,  622, 1013,  295, 8564,   79,\n",
       "           252,  295, 8564,   79,  188,  961,  194,  295, 8564,  293, 3032,  481,\n",
       "            77,  263,   77,  289,  289,  289,  289,  293, 3032,  293, 3032,  364,\n",
       "           293, 3032,  293, 3032,  364,  216, 8564,  295,  756,  204, 8564,  295,\n",
       "           311,  364,  216, 8564,  295,  425,  204, 8564,  295,  547,  634,  204,\n",
       "          8564,  295,  632,  883,  311,  216, 8564,  295,  632,  311,  216, 8564,\n",
       "           216, 8564,  295,  431,  311,  216, 8564,  295,  295,  252, 8564,  311,\n",
       "           216, 8564,  295,  431,  311,  216, 8564,  293]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " tensor([[  0.,   0.,  11.,   8.,   2.,  10.,   4.,   1.,   2.,   6.,   1.,   0.,\n",
       "            0.,  24.,   0.,  16.,   3.,   3.,  22.,  44.,  34.,  16.,  24.,  10.,\n",
       "           27.,   0.,   2.,  18.,  11.,   5.,  19.,  13.,  16.,  31.,   5.,   7.,\n",
       "           11.,  40.,   0.,   1.,   0.,   8.,   3.,   0.,   0.,  25.,   0.,   9.,\n",
       "            5.,   5.,  24., 210., 119.,  13.,   0.,  88.]]),\n",
       " tensor([[126,  84,  71,  54,  37, 113,  84,  56,  35,   7,  75, 120,  89,  57,\n",
       "          122,  82, 108,  24, 119,  96,  19,  17,  59,  10, 119,  19, 110,  79,\n",
       "           28,  38,  63,  62]]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = tokenized_data['test'][0]\n",
    "input_ids = torch.tensor(sample['input_ids'], dtype=torch.long).unsqueeze(0)\n",
    "attention_mask = torch.tensor(sample['attention_mask'], dtype=torch.long).unsqueeze(0)\n",
    "autophase = torch.tensor(sample['autophase'], dtype=torch.float32).unsqueeze(0)\n",
    "labels = torch.tensor(sample['labels'], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "input_ids, attention_mask, autophase, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c16d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xucong24/Compiler/.venv/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:555: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/home/xucong24/Compiler/.venv/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:575: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=tensor(4.8353, grad_fn=<NllLossBackward0>), logits=tensor([[[ 0.5258, -0.1056,  0.2954,  ...,  0.0416,  0.2911,  0.7247],\n",
       "         [ 0.8534, -0.8265,  0.1497,  ..., -0.2997,  0.4178,  1.2306],\n",
       "         [ 0.3849, -0.3823, -0.0124,  ..., -0.0802, -0.3761,  0.9375],\n",
       "         ...,\n",
       "         [ 0.2424, -0.6945, -0.2562,  ...,  0.0376,  0.1115,  1.3001],\n",
       "         [ 0.6673, -0.2131, -0.1713,  ..., -0.7453,  0.0637,  1.1720],\n",
       "         [ 0.4946,  0.1214,  0.0904,  ..., -0.5475, -0.3116,  1.1780]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=EncoderDecoderCache(self_attention_cache=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), cross_attention_cache=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer])), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-8.5094e-01,  9.8385e-01, -1.8286e+00,  ..., -1.3813e+00,\n",
       "           1.9248e+00,  5.0953e+00],\n",
       "         [-7.7419e-01, -1.0314e+00,  5.9179e-01,  ..., -7.8700e-01,\n",
       "           1.7165e+00,  2.7508e+00],\n",
       "         [-8.0521e-01, -2.3166e+00, -8.8293e-01,  ..., -1.2971e+00,\n",
       "           1.4879e+00,  3.4037e+00],\n",
       "         ...,\n",
       "         [-3.4500e-03,  2.7813e-01,  1.6077e+00,  ..., -1.0391e+00,\n",
       "           2.4823e+00,  7.2241e-01],\n",
       "         [ 2.0140e-01, -2.4401e-01, -2.5802e-01,  ..., -7.1144e-01,\n",
       "           1.0739e+00,  2.4898e+00],\n",
       "         [-1.9236e+00,  1.4532e+00, -6.8863e-01,  ..., -4.8435e-01,\n",
       "           1.9806e+00,  1.5125e+00]]], grad_fn=<AddBackward0>), encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(\n",
    "    input_ids=input_ids, \n",
    "    attention_mask=attention_mask, \n",
    "    autophase=autophase, \n",
    "    labels=labels\n",
    ")\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "90cc374b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:127 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[126,  84,  57,  57,  57,  57,  57,  57,  57,  57,  57,  57,  57,  57,\n",
       "          57,  57,  57,  57,  57,  24,  24]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(inputs=input_ids, autophase=autophase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "de213ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xucong24/Compiler/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/xucong24/Compiler/.venv/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:555: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/home/xucong24/Compiler/.venv/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:575: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:03, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=4.693825721740723, metrics={'train_runtime': 5.8725, 'train_samples_per_second': 1.362, 'train_steps_per_second': 0.341, 'total_flos': 1029195694080.0, 'train_loss': 4.693825721740723, 'epoch': 2.0})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['test']\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a39b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"passformer_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec4ce447",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PassformerModel.from_pretrained(\"passformer_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65814489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e04da7196d44949437a2889d313a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8d7e2354d04506942b990691db42cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/33 shards):   0%|          | 0/32916 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "data = load_from_disk(\"/home/xucong24/Compiler/datasets/ga_llvm_37k\")\n",
    "data = data.train_test_split(test_size=0.1, seed=42)\n",
    "data.save_to_disk(\"/home/xucong24/Compiler/datasets/ga_llvm_37k_splited\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57cabc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
