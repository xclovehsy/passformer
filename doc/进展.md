## 01.10

**Encoder-Decoder 模型搭建与训练**

这周搭建 Encoder-Decoder 模型用于生成 LLVM 优化 Pass 序列：

- Encoder 用的是之前预训练好的 ModernBERT

- Decoder 用的是 6 层 Transformer


在 IR-Pass 数据集上跑了一下，发现 loss 下降特别快，几乎瞬间就降到 0 了，明显不正常。

排查了一下，发现两个问题：

- 标签处理有问题：padding 部分没有屏蔽掉，模型在学习预测大量无意义的 pad token，所以 loss 看起来降得很快

- 数据收集脚本有 bug：之前用多进程收集数据，保存顺序乱了，导致相似的样本挤在一起


这两个问题都修了，下一步重新跑一下数据集和训练看看效果。

**CompilerDream 复现**

下载了 CompilerDream 的代码和数据集，正在复现实验结果并学习代码，后续会参考这篇工作搭建强化学习环境。



## 01.13

数据集预处理

原始IR-Pass数据集数据格式

```
DatasetInfo(
	features={
		'Benchmark': Value('string'), 			# benchmark
		'CpuInfo': {							# 硬件信息
			'cores_count': Value('int64'), 
			'l1d_cache_count': Value('int64'), 
			'l1d_cache_size': Value('int64'), 
			'l1i_cache_count': Value('int64'), 
			'l1i_cache_size': Value('int64'), 
			'l2_cache_count': Value('int64'), 
			'l2_cache_size': Value('int64'), 
			'l3_cache_count': Value('int64'), 
            'l3_cache_size': Value('int64'), 
            'l4_cache_count': Value('int64'), 
            'l4_cache_size': Value('int64'), 
            'name': Value('string')}, 
		'IrInstructionCountO0': Value('int64'), 	# 未优化IR行数
		'IrInstructionCountO3': Value('int64'), 	# 执行O3优化的IR行
		'IrInstructionCountOz': Value('int64'), 	# 执行Oz优化后的IR行
		'InstCount': List(Value('int64')), 			# instcount
		'Autophase': List(Value('int64')), 			# autophase
		'Reward': Value('float64'), 				# pass优化得分
		'Commandline': Value('string'), 			# 优化pass
		'LLVM_IR': Value('string')					# IR
	}, 
)
```

样本sample

```
{'Benchmark': 'generator://csmith-v0/10018', 
'CpuInfo': {'cores_count': 33, 'l1d_cache_count': 33, 'l1d_cache_size': 49152, 'l1i_cache_count': 33, 'l1i_cache_size': 32768, 'l2_cache_count': 33, 'l2_cache_size': 1310720, 'l3_cache_count': 2, 'l3_cache_size': 50331648, 'l4_cache_count': 0, 'l4_cache_size': -1, 'name': 'Intel Xeon Platinum 8358'}, 
'IrInstructionCountO0': 189, 
'IrInstructionCountO3': 130, 
'IrInstructionCountOz': 122, 
'InstCount': [189, 24, 12, 8, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 11, 0, 5, 15, 36, 23, 3, 0, 0, 0, 9, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 6, 0, 0, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
'Autophase': [0, 0, 11, 8, 2, 10, 4, 1, 2, 6, 1, 0, 0, 24, 0, 16, 4, 3, 22, 38, 31, 11, 22, 10, 28, 0, 3, 15, 11, 4, 20, 10, 16, 27, 3, 6, 11, 36, 0, 0, 0, 8, 1, 0, 0, 23, 0, 9, 5, 5, 24, 189, 104, 12, 0, 76], 
'Reward': 1.4022988505747127, 
'Commandline': '-lower-expect -loop-rotate -newgvn -insert-gcov-profiling -licm -mergereturn -lcssa -strip-debug-declare -barrier -gvn -strip-nondebug -lower-widenable-condition -licm -tailcallelim -loweratomic -rewrite-statepoints-for-gc -dse -strip-debug-declare -globalopt -loop-deletion -dce -load-store-vectorizer -loop-unroll-and-jam ... ', 
'LLVM_IR': '; ModuleID = \'-\'\nsource_filename = "-"\ntarget datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"\ntarget triple = "x86_64-unknown-linux-gnu"\n\n@.str = private unnamed_addr constant [2 x i8] c"1\\00", align 1\n@g_2 = internal global i32 1, align 4\n@.str.1 = private unnamed_addr constant [4 x i8] c"g_2\\00", align 1\n@crc32_context = internal global i32 -1, align  ... '}
```

**模型训练**

将构建的Encoder-Decoder模型在IR-Pass数据集上做监督学习。

- encoder_maxlen: 1024
- decoder_maxlen: 256
- epochs: 20  
- lr: 5e-5

![passformer_train_20epoch](https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/passformer_train_20epoch.png)

**方法对比**

| Method | Walltime | Code size Reduction |
| --- | --- | --- |
| CompilerDream + Guided Search | 60.8s | 1.073× |
| PPO + Guided Search | 69.8s | 1.070× |
| CompilerDream | 2.9s | 1.068× |
| **Passformer** | - | **0.408×** |
| Random Search (t=10800) | 10,512.4s | 1.062× |
| Random Search (t=3600) | 3,630.8s | 1.061× |
| Greedy Search | 169.2s | 1.055× |
| GATv2 + DD-PPO | 258.1s | 1.047× |

模型评估

| benchmark | original_ir | optimized_ir | o3_ir | relative_to_o3 | pass |
| --- | --- | --- | --- | --- | --- |
| cbench-v1_adpcm | 567 | 262 | 224 | 0.8892 | libcalls-shrinkwrap, simple-loop-unswitch, guard-widening, memcpyopt, lower-constant-intrinsics, gvn-hoist, loop-load-elim, correlated-propagation, loop-rotate, add-discriminators |
| cbench-v1_bitcount | 857 | 1036 | 697 | -1.1187 | libcalls-shrinkwrap, loop-simplify, instnamer, ipconstprop, lower-matrix-intrinsics, name-anon-globals, rpo-functionattrs, sancov, barrier, loop-reduce |
| cbench-v1_blowfish | 3898 | 2061 | 1954 | 0.9450 | loop-reduce, attributor, loop-fusion, mem2reg, lower-matrix-intrinsics, ee-instrument, called-value-propagation, scalarizer, alignment-from-assumptions, gvn |
| cbench-v1_bzip2 | 28748 | 14850 | 22098 | 2.0899 | libcalls-shrinkwrap, simple-loop-unswitch, guard-widening, memcpyopt, lower-constant-intrinsics, gvn-hoist, loop-load-elim, correlated-propagation, loop-rotate, add-discriminators |
| cbench-v1_crc32 | 242 | 316 | 193 | -1.5102 | libcalls-shrinkwrap, loop-simplify, instnamer, ipconstprop, lower-matrix-intrinsics, name-anon-globals, rpo-functionattrs, sancov, barrier, loop-reduce |
| cbench-v1_dijkstra | 450 | 338 | 368 | 1.3659 | libcalls-shrinkwrap, simple-loop-unswitch, guard-widening, memcpyopt, lower-constant-intrinsics, gvn-hoist, loop-load-elim, correlated-propagation, loop-rotate, add-discriminators |
| cbench-v1_ghostscript | 406198 | 429132 | 317976 | -0.2600 | dse, lower-matrix-intrinsics, callsite-splitting, strip-dead-prototypes, ipsccp, loop-deletion, slp-vectorizer, memcpyopt, globalsplit, aggressive-instcombine |
| cbench-v1_gsm | 14902 | 14700 | 10660 | 0.0476 | loop-versioning, loop-simplify, lowerinvoke, loop-instsimplify, sancov, irce, rpo-functionattrs, mergeicmps, correlated-propagation, break-crit-edges |
| cbench-v1_ispell | 15184 | 19354 | 13178 | -2.0788 | dse, loop-reduce, callsite-splitting, adce, loop-simplify, ipconstprop, slp-vectorizer, instsimplify, callsite-splitting, nary-reassociate |
| cbench-v1_jpeg-c | 62452 | 62851 | 54821 | -0.0523 | loop-unswitch, loop-fusion, inferattrs, loop-deletion, div-rem-pairs, aggressive-instcombine, correlated-propagation, reassociate, early-cse, irce |
| cbench-v1_jpeg-d | 59942 | 60865 | 54435 | -0.1676 | loop-unswitch, loop-fusion, inferattrs, loop-deletion, div-rem-pairs, aggressive-instcombine, correlated-propagation, reassociate, early-cse, irce |
| cbench-v1_lame | 49131 | 39054 | 42663 | 1.5580 | dse, lower-matrix-intrinsics, callsite-splitting, adce, loop-simplify, loop-deletion, slp-vectorizer, instsimplify, callsite-splitting, nary-reassociate |
| cbench-v1_patricia | 1282 | 452 | 569 | 1.1641 | libcalls-shrinkwrap, simple-loop-unswitch, guard-widening, memcpyopt, lower-constant-intrinsics, gvn-hoist, loop-load-elim, correlated-propagation, loop-rotate, add-discriminators |
| cbench-v1_qsort | 638 | 329 | 2323 | 1.0000 | libcalls-shrinkwrap, simple-loop-unswitch, guard-widening, memcpyopt, lower-constant-intrinsics, gvn-hoist, loop-load-elim, correlated-propagation, loop-rotate, add-discriminators |
| cbench-v1_rijndael | 6133 | 3931 | 3297 | 0.7764 | libcalls-shrinkwrap, loop-simplify, instnamer, ipconstprop, lower-matrix-intrinsics, name-anon-globals, rpo-functionattrs, sancov, barrier, loop-reduce |
| cbench-v1_sha | 799 | 369 | 1295 | 1.0000 | libcalls-shrinkwrap, simple-loop-unswitch, guard-widening, memcpyopt, lower-constant-intrinsics, gvn-hoist, loop-load-elim, correlated-propagation, loop-rotate, add-discriminators |
| cbench-v1_stringsearch | 1348 | 707 | 1490 | 1.0000 | libcalls-shrinkwrap, simple-loop-unswitch, guard-widening, memcpyopt, lower-constant-intrinsics, gvn-hoist, loop-load-elim, correlated-propagation, loop-rotate, add-discriminators |
| cbench-v1_stringsearch2 | 482 | 256 | 281 | 1.1244 | libcalls-shrinkwrap, simple-loop-unswitch, guard-widening, memcpyopt, lower-constant-intrinsics, gvn-hoist, loop-load-elim, correlated-propagation, loop-rotate, add-discriminators |
| cbench-v1_susan | 13282 | 5952 | 6459 | 1.0743 | libcalls-shrinkwrap, simple-loop-unswitch, guard-widening, memcpyopt, lower-constant-intrinsics, gvn-hoist, loop-load-elim, correlated-propagation, loop-rotate, add-discriminators |
| cbench-v1_tiff2bw | 58797 | 53198 | 44539 | 0.3927 | loop-versioning, lower-matrix-intrinsics, loop-distribute, lower-widenable-condition, loop-deletion, sccp, speculative-execution, loop-predication, infer-address-spaces, jump-threading |
| cbench-v1_tiff2rgba | 58661 | 52154 | 44256 | 0.4517 | loop-versioning, lower-matrix-intrinsics, loop-distribute, lower-widenable-condition, loop-deletion, sccp, speculative-execution, loop-predication, infer-address-spaces, jump-threading |
| cbench-v1_tiffdither | 58450 | 47353 | 44412 | 0.7905 | loop-versioning, lower-matrix-intrinsics, loop-distribute, lower-widenable-condition, loop-deletion, sccp, speculative-execution, loop-predication, infer-address-spaces, jump-threading |
| cbench-v1_tiffmedian | 61265 | 78171 | 45820 | -1.0946 | loop-unswitch, loop-fusion, inferattrs, loop-deletion, div-rem-pairs, aggressive-instcombine, correlated-propagation, reassociate, early-cse, irce |

总结：

本周主要完成了以下几项工作：

1. **数据集预处理与模型训练**：重新预处理了IR-pass数据集，并训练了encoder-decoder模型。从TensorBoard监控来看，模型训练loss下降正常，测试集loss持续下降。目前已完成20个epoch的训练，测试集loss趋于稳定（正在继续训练至40个epoch）。

2. **评估方法构建**：由于compiler_gym依赖低版本的gym，导致compiler_gym与支持modernbert版本的transformers库无法部署在同一环境中，因此我手动利用LLVM工具库实现了优化pass的代码缩减比例计算。

从评估结果来看，passformer相较于SOTA方法还存在一定差距，分析主要有以下两个原因：

1. **IR长度限制**：部分benchmark程序的IR行数超过5万行，远远超过modernbert的最大窗口8192。考虑到单卡显存限制，监督学习阶段设置窗口为1024（目前正在尝试encoder_max=8192训练，但显存要求很高，训练进度较慢）。modernbert无法提取完整IR的语义特征。

2. **学习方法局限**：仅依靠监督学习难以达到启发式搜索和强化学习的优化效果。

**下一步工作计划**：

1. 设计模型能够感知IR的静态特征（如instcount、autophase），目前正在调研实现方法。
2. 搭建类似RLHF的强化学习框架，将优化pass的得分作为reward信号。 


老师，我同步一下最近几天进展

环境冲突问题解决
之前compiler_gym和transformers库版本冲突的问题。在官网上找到支持python3.10的稳定版本wheel包，通过手动安装并修复了对numpy2.0的兼容性问题。

模型训练进展
在之前的EncoderDecoder模型基础上，新增了程序的Autophase统计特征融合。具体是将56维的Autophase特征通过MLP投影到模型的隐藏维度，将Autophase特征与Encoder输出的语义特征做concat/add，再输入Decoder做Pass推理。目前正在训练中，预计还需要两天左右时间
引入Autophase静态统计特征，可以在无法完整编码长IR的情况下，仍能感知到程序的关键结构信息，能一定程度解决之前提到的IR长度限制问题。

Function Level方案调研
我也调研了一下function level的优化方案，这个思路是对程序中的每个函数独立进行优化。但相关工作的代码还没有开源，需要修改CompilerGym的底层调用，实现成本比较大。
其次这种方法的优化时间和函数数量成正比，和端到端一次生成优化pass序列的目标不太一致。其次这篇工作也筛选了部分代码规模较小的Benchmark做训练测试，对于这些已经可以塞下ModernBert上下文窗口了。
目前没有现成的function level数据集和Benchmark，如果要推进需要自己构建。所以我想现阶段先专注模型训练和后续的强化学习工作，可以挑选一些代码规模相对较小的benchmark进行验证。等工作完善后，再考虑尝试function level的调优方案。


修复bug
训练集loss是测试集8倍
排查dropout
验证训练集和测试集label分布
初始方案是继承EncoderDecoderModel
现在方案继承PretrainModel 重新实现
待观察
gradient_accumulation_steps


